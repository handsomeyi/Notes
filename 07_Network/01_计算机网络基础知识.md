# **网络模型**

七层模型: 应用层(Http, FTP), 表示层(Telnet), 会话层(DNS), 传输层, 网络层, 数据链路层, 物理层
五层模型: 应用层(http, DNS, FTP, SMTP, telnet), 传输层(TCP, UDP), 网络层(IP, ICMP, ARP), 数据链路层(ppp), 物理层
TCP/IP四层模型: 数据链路与物理层合称网络接口层(ppp)

# **http与https的区别？**  

http不安全，明文传输，端口80，地址http://，运行在tcp之上。
https安全，混合加密传输，端口443，地址https://，相当于是身披SSL的http，https运行于ssl之上，ssl运行于tcp之上。
https比http更安全，但是由于**加密解密**更耗资源。

# **http请求与响应报文格式？**  

请求：请求行（请求方法+http版本+url地址）、请求头（key-value键值对）、空格、请求体
响应：状态行（状态码+状态值+http版本）、响应头（key-value键值对）、空格、响应头

# **http常见状态码以及请求头？**

200、301（永久重定向）

302（临时重定向）

400（请求报文语法错误）

401（授权失败，需要身份验证）

404（请求资源找不到）

500（内部服务器错误）、502（网关错误）、504（网关超时，超时得不到响应）
http请求头：

Connection: keep-alive（开启长连接）

User-Agent（客户端使用的操作系统和浏览器的名称和版本）

Accept：text/html（浏览器可接受类型）

Accept-Language、Host(被请求资源的Internet主机和端口号)

# **长连接与短链接？** 

长连接：一次TCP连接，多次Http通信
短连接：一次连接，一次通信

# **cookie与session?**  

cookie一般存用户信息（**Token**）

session一般通过服务器记录用户状态（购物车）

cookie工作机制：当浏览器第一次访问服务端，服务端创新cookie，可将用户信息存于其中，然后返回给浏览器，下一次访问服务端携带cookie，可以得到用户信息。

session工作机制：当浏览器第一次访问服务端，会创建session，还会创建一个特殊的cookie，name为JsessionId，value为sessionid，返回cookie给浏览器，之后访问服务端携带cookie，取出sessionid得到session。

# **redis session共享？**  

由于分布式环境下，对台服务器，当客户端请求服务器A，创建session，返回携带sessionid的cookie给客户端，下次客户端访问服务器B根据sessionid得不到session，就有问题。可以用redis解决，比如将用户信息存入redis，key为sessionid，返回携带sessionid的cookie给客户端，下次访问携带cookie，取出sessionid从redis取出相关信息。

# **禁止cookie怎么办？** 

url后附带sessionid

# http1.1新特新(比较http1.0)？  

**支持断点续传**，对**带宽进行了优化**，不用每次只要对象一部分而把整个对象传过来（分块传输）支持长连接。
流水线（piping管道传输），使得一次TCP连接可以多次HTTP通信，并且可以使得发起一次HTTP请求后不用等响应就可以继续发下一次（虚假并行）。
将客户端的队列传输移到了服务端，服务端需要队列顺序处理请求返回，这样就可能造成队头阻塞，HTTP2.0采用多路复用解决了
引入了更多的状态码，比如206断点传输返回；100，使得不用每次请求都发body，可以先试探的发一个header，看是否有权限，若有返回100，下一次发带body的请求，没有权限返回401.
引入了更多的缓存策略，如if-match
默认开启长连接keep-alive，增加HOST域（一台服务器多个虚拟主机）
缺点：
1、没有头部压缩，服务器端按队列顺序处理请求
2、队头阻塞（多路复用）
3、服务器只能被动响应，2.x引入主动推送

# http1.1与http2.0?

**头部压缩**（2之前body有对应压缩算法，但是header没有，header里面有大量固定字段，如Accept cookie等，大量请求下，会有很多冗余字段，2引入Hpack压缩算法对头部进行压缩）
**多路复用**（由于1.0 虽然也引入了流水线 长连接 提高了效率，但是服务端只能按队列顺序处理请求返回，会造成对头阻塞）2.0引入二进制帧，stream流，一个tcp连接里面可以有多个stream流，请求可以再各自strem里面并发执行，stream里面请求响应消息由一个个帧组成，带有strem id，所以不同stream里面帧可以乱序发送，接收端可以根据streamid按序拼接。
服务器推送 比如请求html，会直接把你需要的js css img传给你 不用自己再去请求一次（可以通过nginx配置）

# **http常见字段  ？**

- **Request Header**
  	○Host：访问服务器的域名
  	○Accept：客户端支持的数据格式（text/html）
  	○Connection：TCP连接复用（长连接）keep-alive
  	○Accept-Encoding：客户端支持数据压缩格式

- **Response Header**
  	○Content-type：响应数据格式（对应Accept）
  	○Content-length：响应数据长度
  	○Content-Encoding：数据压缩格式（gzip）

# **HTTP方法有哪些？哪些是幂等的？Resultful风格方法有哪些？**

http方法有哪些：get、post、delete(删除某个资源)、head(只传head，不穿body)、put(创建更新资源)，options(描述目标资源的通信选项，支持什么http策略)、trance（主要用于测试与诊断，回显服务器收到的请求）

幂等：GET，HEAD，OPTIONS，TRACE，PUT和DELETE，但是put delete不安全 post不幂等

Restfule风格是一种软件架构风格，可以使得，get/post/delete/put等方式对请求的处理方法进行区分

# **https连接过程？**  

●客户端发送随机数+支持的加密算法+支持的SSL版本等信息致服务端
●服务端选择协议版本，加密算法，然后和证书与随机数发给客户端
●客户端验证CA证书合法性（CA公钥事先已经置于浏览器或操作系统）
●客户端产生（随机数）预主密匙并用证书公钥加密发给服务端
●客户端告知服务端将使用加密通信，用协商的加密算法与通信密匙（通信密匙=客户端随机数+服务端随机数+预主密匙）
●客服端发送消息给服务端，让其验证客服端是不是刚才建立信任的客户端（之前全部发送的内容做个摘要）
●服务端收到加密后的预主密匙用CA私钥解密，再结合它的随机数与客户端随机数生成通信密匙
●服务端验证客户端发来验证消息，无误告知客户端使用加密通信，也向客户端发送验证消息
●客户端验证服务端发来的消息，
●无误后，之后开始加密通信

# **ARP作用？**

ip地址到mac地址的转化。

# **DDos攻击？**  

分布式拒绝服务，发起大量请求，大规模消耗目标网站的资源，使其无法正常服务。

# TCP与UDP区别及场景？（首部格式、大小、场景）

如何理解?

**TCP就像微信发短信发视频一样, 不会出现丢包等情况, 面向连接的有拥塞控制, 可以通过发送是否成功而通过滑动窗口来控制发送速度.** => ==提供可靠交付==

**UDP就是打视频, 有时候会卡顿, 丢包......**

tcp可靠传输，面向字节流，拥塞控制，流量控制，一对一

udp不可靠传输，面向报文，没有拥塞控制，流量控制，支持一对一，一对多

tcp首部长度若没有选项字段大小20字节，udp首部长度固定8字节（这也是为何udp没有首部长度字段）

 使用场景： 

tcp效率要求低，准确性要求高的（文件传输）；udp效率要求高，准备要求相对可以低点（QQ聊天，视频通话）

 为何udp快： 

不需要建立连接、没有超时重传、对收到数据不需给确认、没有流量控制拥塞控制

 tcp首部格式： 

20字节源端口、目的端口、序列号、确认号、数据偏移、保留字、URG、ACK、FIN、PSH、FIN

 udp首部格式： 

首部8字节，源端口、目的端口、包长度、检验和，再计算检验和时还会添加12字节的伪首部

# **UDP怎样实现可靠传输？**  

可以将可靠传输服务转移到应用层中实现。

- 顺序问题
- 丢包问题
- 流量控制
- 拥塞控制

1. 应用层中增加seq/ack机制，确保数据发送到对端(保证可靠)

2. 添加超时重传机制 => 拥塞控制 (丢包问题)
3. **拥塞控制**: 用来避免包丢失和超时重传

​	- **慢启动**: 发送一个报文, 如果成功, 则倍增报文段, 否则重新从一个报文段数据开始

​	- **快速重传算法**: 简单来说就是**拥塞窗口减半**，**后续线性增速**。

# **打开一个网址用到的协议？** 

TCP、IP、http、ARP(ip => mac地址)、OSPF(IP数据包在路由器上的路由器选择)、DNS(域名 => 目标IP)

# **拥塞控制？**  

作用：流量控制是防止接收方来不及接收，拥塞控制是防止发送方数据填满网络，出现网络拥塞
拥塞窗口（cwnd）：发送方维护的一个状态量，会根据网络拥塞情况动态变化
swnd=min(rwnd，cwnd)

- 慢开始：开始cwnd=1，试探性发送一个报文，若收到确认ACK，cwnd翻倍（2 4 8 16），如此反复，当cwnd>=ssthresh，使用拥塞避免，之后cwnd线性增长，每次+1.

- 拥塞避免：每当收到⼀个 ACK 时，cwnd 增加 1/cwnd，因此相当于拥塞避免是呈线性增长。一直增长就会出现拥塞，出现丢包，促发重传，进入拥塞发生

- 快重传

  - 接收方收到失序的报文段，也就是出现包丢失，会对该包发起重传请求，若发送方连续收到3个重复请求，报文段丢失，马上快速重传

  - cwnd=cwnd/2，ssthresh=cwnd，进入快速恢复

- 快恢复：速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。

  - cwnd=ssthresh+3(3个数据包已经收到)

  - 重传丢失数据包

  - 若再收到重复ACK,cwnd+1，

  - 若收到新数据ACK,cwnd设置为第一步ssthresh的值，状态已恢复，进入拥塞避免。

# **流量控制？**

作用：防止发送方无脑发送数据，但接收方处理不过来，导致触发重传机制，无端浪费网络流量
过程：TCP首部窗口字段，Window，接收方可以通过该字段告知发送方自己还有多少缓冲区可以接收数据，以此控制发送方速率

# **TCP三次握手？**  

开始客户端服务端都处于CLOSE状态，然后服务端主动监听某个端口，处于listen状态

- 客户端发送SYN报文：客户端随机初始化seq=x，并将序列号置于TCP首部序列号字段中，并将SYN标志位置为1，发送SYN报文到服务端，客户端变为SYN_SEND

- 服务端发送SYN+ACK报文：服务端收到客户端的SYN报文，也会随机初始化序列号seq=y，并将序列号置于TCP首部序列号字段，还会将x+1置于确认应答号字段中，然后置SYN=1,ACK=1将报文发给客户端，不包含应用层数据，之后服务端处于SYN_REVD状态

- 客户端发送ACK报文：客户端收到服务端报文，向服务端发送确认的确认，置ACK标志位为1，确认应答号为y+1，然后发给服务端，这次可以携带数据，此后，客户端服务端都处于ESTABLISHED状态。

客户端发起连接请求报文段（客：SYBN_SEND）
服务端为该TCP连接分配缓存与变量，给予确认（服：SYN_REVD）
客户端为该TCP连接分配缓存与变量（客：ESTABLISHED）,给予确认的确认（服：ESTABLISHED）

# **如何在Linux查看TCP状态？**

netstat -natp

# 为什么三次握手?

- **三次握⼿才可以阻⽌历史连接的重复初始化**（主要原因）：由于某些原因当旧的连接比新的连接先到达服务端，服务端回发一个SYN+ACK报文，客户端收到后与自己上下文比对（序列号），发现这是历史连接（序列号过期或超时），便会发送RST报文到服务端终止这次连接。如果是历史连接（序列号过期或超时），则第三次握⼿发送的报⽂是 RST 报⽂，以此中⽌历史连接，而两次握手不行。

- **同步双方序列号**：TCP通信双方，需维护序列号来保证可靠传输（数据去重，有序等），客户端发送初始化序列号给服务端，服务端收到也要发一个给客户端，客户端收到也要对服务端回复一个确认，两次握手只能保证一方初始化序列能被接收，不能保证两方都可，不可靠

- **数据传输可靠性**：服务端接收客户端数据给出确认，代表服务端有接收数据与发送数据能力，不知道客户端有没有接收数据能力，数据不可靠。

- **防止资源浪费**：某个连接请求因网络延迟，延至到下一个请求正常连接释放后到达服务端，服务端给出确认，如果两次握手代表连接建立，但是客户端此时不会理会这个延迟请求了，服务端就一致超时重传，浪费资源。也可能客户端的 SYN 阻塞了，重复发送多次 SYN 报⽂，只有两次握手客户端收到报文不会给出确认，那么服务端不知道客户端收到自己发的报文没，那么服务器在收到请求后就会建⽴多个冗余的⽆效链接，造成不必要的资源浪费

# **TCP四次挥手？** 

客服端发起连接释放报文，TCP首部FIN字段置为1，发送FIN报文（客：FIN_WAITE1）
服务端接收并回传一个确认报文段,ACK=1（服：CLOSE_WAITE 客：FIN_WAITE2）
服务端处理完发起连接释放报文，置FIN=1，TCP主动关闭（服：LAST_ACK）
客户端收到FIN报文，给予回复ACK报文（客：TIME_WAITE）
服务端收到ACK报文，进入CLOSED状态,服务端完成连接释放
客户端等2MSL后，进入CLOSED状态，客户端完成连接释放

服务器端在发送完FIN后还能收到数据吗？
当收到对方的FIN报文时, 仅仅表示对方不在发送数据了, 但是还能接收数据  不发数据，可以接收数据

# **为什么四次挥手？**

客户端释放连接，但是此时服务端可能还有数据要处理，先暂且给客户端一个回复确认，等处理完后在释放连接，所以要四次
被动方此时有可能还有相应的数据报文需要发送

# **为什么要等2MSL（最长报文连接）才彻底释放连接？为什么要Time_wait状态？**  

首先知道MSL意思（最长报文存活时间）

- 防止最后一个ACK被动关闭方没收到，那么被动关闭方可以超时重传，这样一来一回最长就2MSL

- 防止旧连接数据包到达，经过2MSL足够让旧连接报文过期消失



# **TCP怎样保证可靠传输的？**

**校验和**

**确认应答**

seq机制可以确定接收方数据包有序

**序列和**

**超时重传**

**流量控制**

**拥塞控制**

# **DNS使用什么协议？**  

53端口；

DNS服务器间进行域传输的时候用TCP 53（域名服务器之间进行数据同步，保证数据一致性，要求可靠）；

客户端查询DNS服务器时用 UDP 53，DNS查询超过512字节，TCP标志出现 使用TCP发送。

# **半连接队列与全连接队列？**

长连接：一次TCP连接，多次Http通信
短连接：一次连接，一次通信

# **SYN攻击？**

SYN攻击，就是伪造大量虚假IP发起连接，请求，服务端返回SYN+ACK报文，

由于**IP虚假不会给服务端确认应答**，一直便处于半连接状态置于半连接队列，等队列占满，无法给正常连接服务。

# **tcp的粘包问题怎么解决？**

TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，

从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾

粘包产生的原因：

1. 发送的**数据小于TCP发送缓冲区**的大小。

2. TCP将多次写入缓冲区的数据一次发送出去，将会发送粘包；

3. 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。

4. 要发送的数据大于TCP缓冲区剩余空间的大小，将发生拆包。

**解决方法**：

- 使用带**消息头的协议**，消息头存储消息开始标志及消息长度；

- **消息定长**，每次发送固定长度消息；

- 设置消息边界，**界限符分割**；

- 更**复杂的应用层协议**

# Time_wait什么时候·产生？为何产生？怎样避免？

**什么时候产生**：首先调用close()发起主动关闭的一方，再发送最后一个ACK之后
**为何产生**：确保最后一个ACK到达，保证TCP全双工连接可靠释放；使旧的数据包过期消失。
**什么时候会产生大量Time_wait**：当请求量比较大的时候，而且所有的请求都是短连接的时候
**如何避免**：

- 多IP增加随机端口；

- 内核参数调优（服务器设置SO_REUSEADDR套接字选项来通知内核，如果端口忙，但TCP连接位于TIME_WAIT状态时可以重用端口）；

- 使用长连接（Connection：keep-alive）、Linux参数net.ipv4.tcp_tw_reuse 和 tcp_timestamps开启，复⽤处于 TIME_WAIT 的 socket 为新的连接所⽤

# **Time_wait过多有什么危害？**  

占着茅坑(socket四元组)不拉屎, 相同的对端不能用这个资源建立新的连接了? 

**内存资源占用**、**端口资源占用**（一个TCP连接至少消耗一个端口），每端口，无法建立新连接。
服务器资源受限：服务器监听一个端口，会把连接丢给线程处理，可以继续监听端口，但是线程池处理不了那么多连接。

可以用socket连接池

或者设置最大socket数量 更多
